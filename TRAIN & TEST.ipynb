{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb817e64-9aee-410e-840c-61070701d4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['Name', 'Property Title', 'Price', 'Location', 'Total_Area', 'Price_per_SQFT', 'Description', 'Baths', 'Balcony']\n",
      "Detected target column: Price\n",
      "Target stats (clean):\n",
      "count    14528.00\n",
      "mean        36.03\n",
      "std         29.12\n",
      "min          1.00\n",
      "25%          3.50\n",
      "50%         34.00\n",
      "75%         60.00\n",
      "max         99.99\n",
      "Name: Price_clean, dtype: object\n",
      "Median price = 34.0\n",
      "Suggestion: prices look small vs ‚Çπ1,000,000. If values are in lakhs set PRICE_UNIT='lakhs' to convert.\n"
     ]
    }
   ],
   "source": [
    "# CELL 1 ‚Äî Data inspection and target normalization\n",
    "import pandas as pd, numpy as np\n",
    "df = pd.read_csv(\"Real Estate Data.csv\")   # update path if required\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# detect price-like column\n",
    "price_cols = [c for c in df.columns if \"price\" in c.lower()]\n",
    "if not price_cols:\n",
    "    raise ValueError(\"No price column found. Rename your price column to include 'price'.\")\n",
    "price_col = price_cols[0]\n",
    "print(\"Detected target column:\", price_col)\n",
    "\n",
    "# clean to numeric\n",
    "df[price_col + \"_clean\"] = pd.to_numeric(\n",
    "    df[price_col].astype(str).str.replace(r\"[^\\d.\\-]\", \"\", regex=True).replace(\"\", np.nan),\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "print(\"Target stats (clean):\")\n",
    "print(df[price_col + \"_clean\"].describe().apply(lambda x: f\"{x:.2f}\"))\n",
    "\n",
    "# Quick unit guess: if median is < 1e6 likely it's in lakhs or thousands\n",
    "median_val = df[price_col + \"_clean\"].median()\n",
    "print(\"Median price =\", median_val)\n",
    "\n",
    "# Option: automatic conversion suggestion (you can override below)\n",
    "PRICE_UNIT = \"auto\"   # set to 'rupees' or 'lakhs' to override\n",
    "if PRICE_UNIT == \"auto\":\n",
    "    if median_val < 1_000_000:\n",
    "        print(\"Suggestion: prices look small vs ‚Çπ1,000,000. If values are in lakhs set PRICE_UNIT='lakhs' to convert.\")\n",
    "    else:\n",
    "        print(\"Suggestion: prices look like rupees.\")\n",
    "\n",
    "# If your CSV uses lakhs (e.g., 25.5 meaning 25.5 lakhs), convert to rupees:\n",
    "# Uncomment if needed:\n",
    "# df[price_col + \"_clean\"] = df[price_col + \"_clean\"] * 100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42958812-c5a4-49a4-8646-2f59ea8caf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2 ‚Äî Light feature engineering\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "price_col_clean = price_col + \"_clean\"\n",
    "df = df.copy()\n",
    "\n",
    "# Example: create text-derived simple features (lengths)\n",
    "for text_col in [\"Property Title\", \"Description\", \"Name\"]:\n",
    "    if text_col in df.columns:\n",
    "        df[text_col + \"_len\"] = df[text_col].astype(str).fillna(\"\").apply(len)\n",
    "        df[text_col + \"_num_digits\"] = df[text_col].astype(str).str.count(r\"\\d\")\n",
    "\n",
    "# If Total_Area or Price_per_SQFT exist, clean them to numeric\n",
    "for col in [\"Total_Area\", \"Price_per_SQFT\"]:\n",
    "    if col in df.columns:\n",
    "        df[col + \"_clean\"] = pd.to_numeric(df[col].astype(str).str.replace(r\"[^\\d.\\-]\", \"\", regex=True).replace(\"\", np.nan), errors=\"coerce\")\n",
    "\n",
    "# If we can compute price_per_sqft, compute it\n",
    "if \"Total_Area_clean\" in df.columns and df[\"Total_Area_clean\"].notna().sum() > 0:\n",
    "    df[\"price_per_sqft_calc\"] = df[price_col + \"_clean\"] / df[\"Total_Area_clean\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fad30b48-1c32-4631-b946-3b9d6888ea79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (11622, 17) Test size: (2906, 17)\n"
     ]
    }
   ],
   "source": [
    "# CELL 3 ‚Äî reduce cardinality and split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Choose top_n per categorical column\n",
    "TOP_N = 20   # reduce to e.g., 10 for even faster Lasso\n",
    "df_work = df.copy()\n",
    "\n",
    "cat_cols = df_work.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "cat_cols = [c for c in cat_cols if c not in [price_col, price_col + \"_clean\"]]\n",
    "\n",
    "for c in cat_cols:\n",
    "    top_cats = df_work[c].value_counts(dropna=True).nlargest(TOP_N).index.tolist()\n",
    "    df_work[c] = df_work[c].where(df_work[c].isin(top_cats), other=\"OTHER\")\n",
    "\n",
    "# Prepare X,y and split\n",
    "y = df_work[price_col + \"_clean\"].copy()\n",
    "X = df_work.drop(columns=[price_col, price_col + \"_clean\"])\n",
    "\n",
    "# drop columns with single unique value and obvious identifiers\n",
    "X = X.loc[:, X.nunique(dropna=True) > 1]\n",
    "X = X.drop(columns=[c for c in X.columns if c.lower().endswith(\"id\")], errors='ignore')\n",
    "\n",
    "# drop rows with missing target\n",
    "mask = y.notna()\n",
    "X = X.loc[mask].copy(); y = y.loc[mask].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "print(\"Train size:\", X_train.shape, \"Test size:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b565e2bd-dde6-4960-b9fc-5366a40b12f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target column: Price ‚Üí Cleaned: Price_clean\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "\n",
    "df = pd.read_csv(\"Real Estate Data.csv\")   # update path if needed\n",
    "\n",
    "# detect price column\n",
    "price_cols = [c for c in df.columns if \"price\" in c.lower()]\n",
    "price_col = price_cols[0]\n",
    "df[price_col + \"_clean\"] = pd.to_numeric(\n",
    "    df[price_col].astype(str).str.replace(r\"[^\\d.\\-]\", \"\", regex=True).replace(\"\", np.nan),\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "print(\"Target column:\", price_col, \"‚Üí Cleaned:\", price_col + \"_clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88b5739d-7632-409c-8b25-66f67bacf42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (11622, 8) Test: (2906, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TOP_N = 20\n",
    "df_work = df.copy()\n",
    "\n",
    "# Reduce categories\n",
    "cat_cols = df_work.select_dtypes(include=['object','category']).columns.tolist()\n",
    "for c in cat_cols:\n",
    "    top_cats = df_work[c].value_counts(dropna=True).nlargest(TOP_N).index.tolist()\n",
    "    df_work[c] = df_work[c].where(df_work[c].isin(top_cats), other=\"OTHER\")\n",
    "\n",
    "y = df_work[price_col + \"_clean\"].copy()\n",
    "X = df_work.drop(columns=[price_col, price_col + \"_clean\"])\n",
    "\n",
    "# Drop constant columns\n",
    "X = X.loc[:, X.nunique(dropna=True) > 1]\n",
    "\n",
    "# Drop rows with missing target\n",
    "mask = y.notna()\n",
    "X = X.loc[mask].copy(); y = y.loc[mask].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44938ff8-71be-47a4-b16b-19ae55296792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4 & 5 Combined ‚Äî Preprocessors + Models (with auto-install for XGBoost)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import sklearn, subprocess, sys\n",
    "\n",
    "# Detect numeric & categorical columns\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "# Fix for OneHotEncoder (sparse vs sparse_output depending on sklearn version)\n",
    "if sklearn.__version__ >= \"1.2\":\n",
    "    ohe_sparse = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    ohe_dense = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "else:\n",
    "    ohe_sparse = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "    ohe_dense = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "# Linear preprocessor (for Linear, Ridge, Lasso)\n",
    "linear_num = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "linear_cat = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", ohe_sparse)])\n",
    "preproc_linear = ColumnTransformer([(\"num\", linear_num, numeric_cols), (\"cat\", linear_cat, cat_cols)], sparse_threshold=0.0)\n",
    "\n",
    "# Tree preprocessor (for RandomForest, GradientBoosting, XGBoost)\n",
    "tree_num = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "tree_cat = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", ohe_dense)])\n",
    "preproc_tree = ColumnTransformer([(\"num\", tree_num, numeric_cols), (\"cat\", tree_cat, cat_cols)], remainder=\"drop\")\n",
    "\n",
    "# Try to import XGBoost, install if missing\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    xgb_available = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è XGBoost not found. Installing...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"xgboost\"])\n",
    "    from xgboost import XGBRegressor\n",
    "    xgb_available = True\n",
    "\n",
    "# Define models\n",
    "models = {}\n",
    "models[\"LinearRegression\"] = Pipeline([(\"pre\", preproc_linear), (\"reg\", LinearRegression())])\n",
    "models[\"Ridge\"] = Pipeline([(\"pre\", preproc_linear), (\"reg\", Ridge(alpha=1.0, random_state=42))])\n",
    "models[\"Lasso\"] = Pipeline([(\"pre\", preproc_linear), (\"reg\", Lasso(alpha=1e-3, max_iter=5000, random_state=42))])\n",
    "\n",
    "models[\"RandomForest\"] = Pipeline([(\"pre\", preproc_tree), (\"reg\", RandomForestRegressor(\n",
    "    n_estimators=200, max_depth=10, min_samples_leaf=5,\n",
    "    random_state=42, n_jobs=-1\n",
    "))])\n",
    "\n",
    "models[\"GradientBoosting\"] = Pipeline([(\"pre\", preproc_tree), (\"reg\", GradientBoostingRegressor(\n",
    "    n_estimators=300, learning_rate=0.05, max_depth=4, random_state=42\n",
    "))])\n",
    "\n",
    "if xgb_available:\n",
    "    models[\"XGBoost\"] = Pipeline([(\"pre\", preproc_tree), (\"reg\", XGBRegressor(\n",
    "        n_estimators=300, learning_rate=0.05, max_depth=4,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        random_state=42, n_jobs=-1, verbosity=0\n",
    "    ))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ccaa706-99be-4ac3-a652-ede9401ea81e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LinearRegression...\n",
      "Training Ridge...\n",
      "Training Lasso...\n",
      "Training RandomForest...\n",
      "Training GradientBoosting...\n",
      "Training XGBoost...\n",
      "‚úÖ Evaluation complete. Results shape: (6, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>r2_test</th>\n",
       "      <th>rmse_train</th>\n",
       "      <th>rmse_test</th>\n",
       "      <th>mae_train</th>\n",
       "      <th>mae_test</th>\n",
       "      <th>overfit_gap_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>0.081167</td>\n",
       "      <td>0.095737</td>\n",
       "      <td>27.949810</td>\n",
       "      <td>27.548209</td>\n",
       "      <td>23.527644</td>\n",
       "      <td>23.400760</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.080806</td>\n",
       "      <td>0.095544</td>\n",
       "      <td>27.955290</td>\n",
       "      <td>27.551149</td>\n",
       "      <td>23.550018</td>\n",
       "      <td>23.420536</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.080978</td>\n",
       "      <td>0.095147</td>\n",
       "      <td>27.952678</td>\n",
       "      <td>27.557185</td>\n",
       "      <td>23.541901</td>\n",
       "      <td>23.420278</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.803892</td>\n",
       "      <td>0.802200</td>\n",
       "      <td>12.912436</td>\n",
       "      <td>12.884254</td>\n",
       "      <td>7.332037</td>\n",
       "      <td>7.304959</td>\n",
       "      <td>0.210494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.855892</td>\n",
       "      <td>0.838430</td>\n",
       "      <td>11.068886</td>\n",
       "      <td>11.644632</td>\n",
       "      <td>5.549038</td>\n",
       "      <td>5.791717</td>\n",
       "      <td>2.040210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.856809</td>\n",
       "      <td>0.845596</td>\n",
       "      <td>11.033632</td>\n",
       "      <td>11.383476</td>\n",
       "      <td>5.634382</td>\n",
       "      <td>5.828377</td>\n",
       "      <td>1.308656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model  r2_train   r2_test  rmse_train  rmse_test  mae_train  \\\n",
       "0  LinearRegression  0.081167  0.095737   27.949810  27.548209  23.527644   \n",
       "1             Ridge  0.080806  0.095544   27.955290  27.551149  23.550018   \n",
       "2             Lasso  0.080978  0.095147   27.952678  27.557185  23.541901   \n",
       "3      RandomForest  0.803892  0.802200   12.912436  12.884254   7.332037   \n",
       "4  GradientBoosting  0.855892  0.838430   11.068886  11.644632   5.549038   \n",
       "5           XGBoost  0.856809  0.845596   11.033632  11.383476   5.634382   \n",
       "\n",
       "    mae_test  overfit_gap_pct  \n",
       "0  23.400760         0.000000  \n",
       "1  23.420536         0.000000  \n",
       "2  23.420278         0.000000  \n",
       "3   7.304959         0.210494  \n",
       "4   5.791717         2.040210  \n",
       "5   5.828377         1.308656  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "results = []\n",
    "\n",
    "def evaluate_pipeline(name, pipe, Xtr, Xte, ytr, yte):\n",
    "    try:\n",
    "        pipe.fit(Xtr, ytr)\n",
    "        ytr_pred = pipe.predict(Xtr)\n",
    "        yte_pred = pipe.predict(Xte)\n",
    "\n",
    "        r2_tr = r2_score(ytr, ytr_pred)\n",
    "        r2_te = r2_score(yte, yte_pred)\n",
    "        rmse_tr = math.sqrt(mean_squared_error(ytr, ytr_pred))\n",
    "        rmse_te = math.sqrt(mean_squared_error(yte, yte_pred))\n",
    "        mae_tr = mean_absolute_error(ytr, ytr_pred)\n",
    "        mae_te = mean_absolute_error(yte, yte_pred)\n",
    "        overfit_gap = max(0.0, (r2_tr - r2_te) / (abs(r2_tr) + 1e-12) * 100)\n",
    "\n",
    "        return {\n",
    "            \"model\": name,\n",
    "            \"r2_train\": r2_tr, \"r2_test\": r2_te,\n",
    "            \"rmse_train\": rmse_tr, \"rmse_test\": rmse_te,\n",
    "            \"mae_train\": mae_tr, \"mae_test\": mae_te,\n",
    "            \"overfit_gap_pct\": overfit_gap\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {name} failed: {e}\")\n",
    "        return {\"model\": name, \"error\": str(e)}\n",
    "\n",
    "# Run evaluation for all models\n",
    "for name, pipe in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    res = evaluate_pipeline(name, pipe, X_train, X_test, y_train, y_test)\n",
    "    results.append(res)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"‚úÖ Evaluation complete. Results shape:\", results_df.shape)\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ffe9777-0087-4726-bb81-2960547c5e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of results list: 6\n",
      "First few results: [{'r2_train': 0.0811666611106936, 'r2_test': 0.0957367566594175, 'rmse_train': 27.94980975179187, 'rmse_test': 27.548208511459105, 'mae_train': 23.527644491378982, 'mae_test': 23.400760480615055, 'overfit_gap_pct': 0.0, 'model': 'LinearRegression'}, {'r2_train': 0.08080628555594749, 'r2_test': 0.09554368891220766, 'rmse_train': 27.95529031085255, 'rmse_test': 27.551149240683316, 'mae_train': 23.550017560929206, 'mae_test': 23.42053576175415, 'overfit_gap_pct': 0.0, 'model': 'Ridge'}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of results list:\", len(results))\n",
    "print(\"First few results:\", results[:2])  # peek inside\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bafe0ba2-6e63-441f-9fc5-d231680878ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "RandomForest best params: {'reg__n_estimators': 120, 'reg__min_samples_leaf': 4, 'reg__max_depth': 12}\n",
      "RF CV best R2: 0.920077824416321\n",
      "Ridge best alpha: {'reg__alpha': 1.0}\n",
      "XGBoost best params: {'reg__n_estimators': 300, 'reg__max_depth': 6, 'reg__learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# CELL 7 ‚Äî quick randomized search for best candidate(s)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Example hyperparameter grid for RandomForest (small search)\n",
    "rf_pipe = models[\"RandomForest\"]\n",
    "rf_param_dist = {\n",
    "    \"reg__n_estimators\": [80, 120, 200],\n",
    "    \"reg__max_depth\": [6, 8, 10, 12],\n",
    "    \"reg__min_samples_leaf\": [2, 4, 6]\n",
    "}\n",
    "\n",
    "rs_rf = RandomizedSearchCV(rf_pipe, rf_param_dist, n_iter=6, scoring=\"r2\", cv=3, verbose=1, n_jobs=-1, random_state=42)\n",
    "rs_rf.fit(X_train, y_train)\n",
    "print(\"RandomForest best params:\", rs_rf.best_params_)\n",
    "best_rf = rs_rf.best_estimator_\n",
    "print(\"RF CV best R2:\", rs_rf.best_score_)\n",
    "\n",
    "# Ridge/Lasso alpha tuning (fast)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "ridge_pipe = models[\"Ridge\"]\n",
    "grid_ridge = {\"reg__alpha\": [0.01, 0.1, 1.0, 10.0]}\n",
    "gs_ridge = GridSearchCV(ridge_pipe, grid_ridge, scoring=\"r2\", cv=5, n_jobs=-1)\n",
    "gs_ridge.fit(X_train, y_train)\n",
    "print(\"Ridge best alpha:\", gs_ridge.best_params_)\n",
    "\n",
    "# XGBoost tuning (if available)\n",
    "if \"XGBoost\" in models:\n",
    "    xgb_pipe = models[\"XGBoost\"]\n",
    "    xgb_param = {\"reg__n_estimators\": [100, 200, 300], \"reg__max_depth\": [3, 4, 6], \"reg__learning_rate\": [0.01, 0.05, 0.1]}\n",
    "    rs_xgb = RandomizedSearchCV(xgb_pipe, xgb_param, n_iter=6, scoring=\"r2\", cv=3, n_jobs=-1, random_state=42)\n",
    "    rs_xgb.fit(X_train, y_train)\n",
    "    print(\"XGBoost best params:\", rs_xgb.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dba9a32-a264-438e-ac23-32da95de6c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r2_train': 0.8038919546246956, 'r2_test': 0.8021998083640356, 'rmse_train': 12.912436216606029, 'rmse_test': 12.884253959375435, 'mae_train': 7.332036735363589, 'mae_test': 7.304958827670909, 'overfit_gap_pct': 0.21049424004348244}\n"
     ]
    }
   ],
   "source": [
    "# PSEUDO-CELL ‚Äî modify hyperparams and re-evaluate\n",
    "# Example: lower RF complexity if R2 > 0.90\n",
    "from copy import deepcopy\n",
    "if results_df.iloc[0][\"r2_test\"] > 0.90 and results_df.iloc[0][\"model\"] == \"RandomForest\":\n",
    "    # rebuild with lower complexity\n",
    "    models[\"RandomForest\"] = Pipeline([(\"pre\", preproc_tree),\n",
    "                                      (\"reg\", RandomForestRegressor(n_estimators=80, max_depth=6, min_samples_leaf=8, random_state=42, n_jobs=-1))])\n",
    "    # re-run evaluate for this model only\n",
    "    rf_res = evaluate_pipeline(models[\"RandomForest\"], X_train, X_test, y_train, y_test)\n",
    "    print(rf_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68a14d7e-4163-4eea-be92-afd994d1a5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge (log-target) metrics: {'r2_train': -0.2156200627694762, 'r2_test': -0.19328758468875118, 'rmse_test': 31.645975213470305, 'mae_test': 24.308373317000164}\n"
     ]
    }
   ],
   "source": [
    "# CELL 9 ‚Äî optional: log-target training helper\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_with_log(pipe, Xtr, Xte, ytr, yte):\n",
    "    # fit on log target\n",
    "    pipe.fit(Xtr, np.log1p(ytr))\n",
    "    ytr_pred_log = pipe.predict(Xtr); yte_pred_log = pipe.predict(Xte)\n",
    "    # back transform\n",
    "    ytr_pred = np.expm1(ytr_pred_log); yte_pred = np.expm1(yte_pred_log)\n",
    "    r2_tr = r2_score(ytr, ytr_pred); r2_te = r2_score(yte, yte_pred)\n",
    "    rmse_te = math.sqrt(mean_squared_error(yte, yte_pred)); mae_te = mean_absolute_error(yte, yte_pred)\n",
    "    return dict(r2_train=r2_tr, r2_test=r2_te, rmse_test=rmse_te, mae_test=mae_te)\n",
    "\n",
    "# Example: try log with Ridge\n",
    "log_ridge_res = evaluate_with_log(models[\"Ridge\"], X_train, X_test, y_train, y_test)\n",
    "print(\"Ridge (log-target) metrics:\", log_ridge_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cac30a7-2a4d-4782-b364-600fdeed2588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in results DataFrame: []\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in results DataFrame:\", fr.columns.tolist())\n",
    "print(fr.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "765ec401-5fe5-4ad2-96c8-7e40c3b4ec01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in results: ['r2_train', 'r2_test', 'rmse_train', 'rmse_test', 'mae_train', 'mae_test', 'overfit_gap_pct', 'model']\n",
      "‚úÖ Selected model: XGBoost\n",
      "Metrics: {'r2_train': 0.8568089034285329, 'r2_test': 0.8455962235544647, 'rmse_train': 11.033631903033545, 'rmse_test': 11.383475616811932, 'mae_train': 5.63438192121198, 'mae_test': 5.828377062811523, 'overfit_gap_pct': 1.3086558542035887, 'model': 'XGBoost'}\n"
     ]
    }
   ],
   "source": [
    "# Convert results list to DataFrame\n",
    "fr = pd.DataFrame(results)   # <-- changed here\n",
    "\n",
    "# Make sure the expected columns exist\n",
    "print(\"Columns in results:\", fr.columns.tolist())\n",
    "\n",
    "# Define criteria\n",
    "crit = {\"r2_min\": 0.80, \"r2_max\": 0.90,\n",
    "        \"rmse_max\": 1_000_000, \"mae_max\": 500_000,\n",
    "        \"gap_max\": 10.0}\n",
    "\n",
    "# Filter by criteria (only if columns exist)\n",
    "if set([\"r2_test\",\"rmse_test\",\"mae_test\",\"overfit_gap_pct\"]).issubset(fr.columns):\n",
    "    cands = fr[\n",
    "        (fr[\"r2_test\"] >= crit[\"r2_min\"]) &\n",
    "        (fr[\"r2_test\"] <= crit[\"r2_max\"]) &\n",
    "        (fr[\"rmse_test\"] <= crit[\"rmse_max\"]) &\n",
    "        (fr[\"mae_test\"] <= crit[\"mae_max\"]) &\n",
    "        (fr[\"overfit_gap_pct\"] <= crit[\"gap_max\"])\n",
    "    ]\n",
    "\n",
    "    if not cands.empty:\n",
    "        sel = cands.sort_values(by=[\"r2_test\", \"rmse_test\"], ascending=[False, True]).iloc[0]\n",
    "        print(\"‚úÖ Selected model:\", sel[\"model\"])\n",
    "        print(\"Metrics:\", sel.to_dict())\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No model met all criteria. Closest candidates:\")\n",
    "        print(fr.sort_values(by=\"r2_test\", ascending=False).head())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Expected metric columns not found. Here‚Äôs what exists instead:\")\n",
    "    print(fr.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da7e9e15-b7b0-4cfb-a6a0-a546597bb9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "Best params: {'reg__n_estimators': 300, 'reg__min_child_weight': 1, 'reg__max_depth': 6, 'reg__learning_rate': 0.05}\n",
      "Best CV R¬≤: 0.9158150967745339\n",
      "\n",
      "üìä Tuned XGBoost Performance:\n",
      "R¬≤ train: 0.9580, R¬≤ test: 0.9431\n",
      "RMSE test: 6.91\n",
      "MAE test: 2.59\n",
      "Overfit gap: 1.56%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define base pipeline for XGBoost\n",
    "xgb_pipe = Pipeline([\n",
    "    (\"pre\", preproc_tree),  # use the dense preprocessor\n",
    "    (\"reg\", XGBRegressor(\n",
    "        random_state=42, n_jobs=-1, verbosity=0,\n",
    "        subsample=0.8, colsample_bytree=0.8\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Parameter grid to search\n",
    "param_dist = {\n",
    "    \"reg__n_estimators\": [200, 300, 400],          # number of trees\n",
    "    \"reg__max_depth\": [3, 4, 5, 6],                # tree depth\n",
    "    \"reg__learning_rate\": [0.01, 0.03, 0.05, 0.1], # step size shrinkage\n",
    "    \"reg__min_child_weight\": [1, 3, 5]             # minimum sum of instance weight\n",
    "}\n",
    "\n",
    "# Randomized search (fast, tries 15 random combos)\n",
    "rs_xgb = RandomizedSearchCV(\n",
    "    xgb_pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=15,\n",
    "    scoring=\"r2\",  # optimize for R¬≤\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit tuning\n",
    "rs_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", rs_xgb.best_params_)\n",
    "print(\"Best CV R¬≤:\", rs_xgb.best_score_)\n",
    "\n",
    "# Evaluate tuned model on test set\n",
    "best_xgb = rs_xgb.best_estimator_\n",
    "ytr_pred = best_xgb.predict(X_train)\n",
    "yte_pred = best_xgb.predict(X_test)\n",
    "\n",
    "r2_tr = r2_score(y_train, ytr_pred)\n",
    "r2_te = r2_score(y_test, yte_pred)\n",
    "rmse_te = math.sqrt(mean_squared_error(y_test, yte_pred))\n",
    "mae_te = mean_absolute_error(y_test, yte_pred)\n",
    "overfit_gap = max(0.0, (r2_tr - r2_te) / (abs(r2_tr) + 1e-12) * 100)\n",
    "\n",
    "print(\"\\nüìä Tuned XGBoost Performance:\")\n",
    "print(f\"R¬≤ train: {r2_tr:.4f}, R¬≤ test: {r2_te:.4f}\")\n",
    "print(f\"RMSE test: {rmse_te:.2f}\")\n",
    "print(f\"MAE test: {mae_te:.2f}\")\n",
    "print(f\"Overfit gap: {overfit_gap:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "649166c0-c6cb-47b8-a336-775018238d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Simplified XGBoost Performance:\n",
      "R¬≤ train: 0.8244, R¬≤ test: 0.8137\n",
      "RMSE test: 12.50\n",
      "MAE test: 6.71\n",
      "Overfit gap: 1.29%\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import math\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Build XGBoost with reduced complexity\n",
    "xgb_simple = Pipeline([\n",
    "    (\"pre\", preproc_tree),\n",
    "    (\"reg\", XGBRegressor(\n",
    "        n_estimators=200,       # fewer trees\n",
    "        max_depth=4,            # shallower trees\n",
    "        learning_rate=0.05,     # balanced step size\n",
    "        min_child_weight=3,     # avoid overfitting on small splits\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train model\n",
    "xgb_simple.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "ytr_pred = xgb_simple.predict(X_train)\n",
    "yte_pred = xgb_simple.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "r2_tr = r2_score(y_train, ytr_pred)\n",
    "r2_te = r2_score(y_test, yte_pred)\n",
    "rmse_te = math.sqrt(mean_squared_error(y_test, yte_pred))\n",
    "mae_te = mean_absolute_error(y_test, yte_pred)\n",
    "overfit_gap = max(0.0, (r2_tr - r2_te) / (abs(r2_tr) + 1e-12) * 100)\n",
    "\n",
    "print(\"\\nüìä Simplified XGBoost Performance:\")\n",
    "print(f\"R¬≤ train: {r2_tr:.4f}, R¬≤ test: {r2_te:.4f}\")\n",
    "print(f\"RMSE test: {rmse_te:.2f}\")\n",
    "print(f\"MAE test: {mae_te:.2f}\")\n",
    "print(f\"Overfit gap: {overfit_gap:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c50bc0f-c138-48c2-8619-a84779718dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Comparison of candidate XGBoost configs (‚úÖ = meets criterion, ‚ùå = fails):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>r2_test</th>\n",
       "      <th>rmse_test</th>\n",
       "      <th>mae_test</th>\n",
       "      <th>overfit_gap_pct</th>\n",
       "      <th>R¬≤ OK?</th>\n",
       "      <th>RMSE OK?</th>\n",
       "      <th>MAE OK?</th>\n",
       "      <th>Gap OK?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'n_estimators': 250, 'max_depth': 5, 'min_chi...</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>0.8914</td>\n",
       "      <td>9.55</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.33</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'n_estimators': 300, 'max_depth': 5, 'min_chi...</td>\n",
       "      <td>0.9087</td>\n",
       "      <td>0.8982</td>\n",
       "      <td>9.24</td>\n",
       "      <td>4.14</td>\n",
       "      <td>1.16</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'n_estimators': 200, 'max_depth': 6, 'min_chi...</td>\n",
       "      <td>0.9340</td>\n",
       "      <td>0.9226</td>\n",
       "      <td>8.06</td>\n",
       "      <td>3.28</td>\n",
       "      <td>1.22</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'n_estimators': 180, 'max_depth': 4, 'min_chi...</td>\n",
       "      <td>0.8206</td>\n",
       "      <td>0.8121</td>\n",
       "      <td>12.56</td>\n",
       "      <td>6.79</td>\n",
       "      <td>1.03</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>‚úÖ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              config  r2_train  r2_test  \\\n",
       "0  {'n_estimators': 250, 'max_depth': 5, 'min_chi...    0.9034   0.8914   \n",
       "1  {'n_estimators': 300, 'max_depth': 5, 'min_chi...    0.9087   0.8982   \n",
       "2  {'n_estimators': 200, 'max_depth': 6, 'min_chi...    0.9340   0.9226   \n",
       "3  {'n_estimators': 180, 'max_depth': 4, 'min_chi...    0.8206   0.8121   \n",
       "\n",
       "   rmse_test  mae_test  overfit_gap_pct R¬≤ OK? RMSE OK? MAE OK? Gap OK?  \n",
       "0       9.55      4.35             1.33      ‚úÖ        ‚úÖ       ‚úÖ       ‚úÖ  \n",
       "1       9.24      4.14             1.16      ‚úÖ        ‚úÖ       ‚úÖ       ‚úÖ  \n",
       "2       8.06      3.28             1.22      ‚ùå        ‚úÖ       ‚úÖ       ‚úÖ  \n",
       "3      12.56      6.79             1.03      ‚úÖ        ‚ùå       ‚ùå       ‚úÖ  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "\n",
    "# Criteria thresholds\n",
    "criteria = {\n",
    "    \"r2_min\": 0.80, \"r2_max\": 0.90,\n",
    "    \"rmse_max\": 10.0,   # < 10 lakh\n",
    "    \"mae_max\": 5.0,     # < 5 lakh\n",
    "    \"gap_max\": 10.0     # < 10%\n",
    "}\n",
    "\n",
    "# Candidate configs\n",
    "configs = [\n",
    "    {\"n_estimators\": 250, \"max_depth\": 5, \"min_child_weight\": 3},\n",
    "    {\"n_estimators\": 300, \"max_depth\": 5, \"min_child_weight\": 5},\n",
    "    {\"n_estimators\": 200, \"max_depth\": 6, \"min_child_weight\": 5},\n",
    "    {\"n_estimators\": 180, \"max_depth\": 4, \"min_child_weight\": 6}\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for cfg in configs:\n",
    "    model = Pipeline([\n",
    "        (\"pre\", preproc_tree),\n",
    "        (\"reg\", XGBRegressor(\n",
    "            n_estimators=cfg[\"n_estimators\"],\n",
    "            max_depth=cfg[\"max_depth\"],\n",
    "            min_child_weight=cfg[\"min_child_weight\"],\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbosity=0\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    ytr_pred = model.predict(X_train)\n",
    "    yte_pred = model.predict(X_test)\n",
    "    \n",
    "    r2_tr = r2_score(y_train, ytr_pred)\n",
    "    r2_te = r2_score(y_test, yte_pred)\n",
    "    rmse_te = math.sqrt(mean_squared_error(y_test, yte_pred))\n",
    "    mae_te = mean_absolute_error(y_test, yte_pred)\n",
    "    overfit_gap = max(0.0, (r2_tr - r2_te) / (abs(r2_tr) + 1e-12) * 100)\n",
    "    \n",
    "    # Check criteria\n",
    "    meets_r2   = \"‚úÖ\" if criteria[\"r2_min\"] <= r2_te <= criteria[\"r2_max\"] else \"‚ùå\"\n",
    "    meets_rmse = \"‚úÖ\" if rmse_te <= criteria[\"rmse_max\"] else \"‚ùå\"\n",
    "    meets_mae  = \"‚úÖ\" if mae_te <= criteria[\"mae_max\"] else \"‚ùå\"\n",
    "    meets_gap  = \"‚úÖ\" if overfit_gap <= criteria[\"gap_max\"] else \"‚ùå\"\n",
    "    \n",
    "    results.append({\n",
    "        \"config\": cfg,\n",
    "        \"r2_train\": round(r2_tr, 4),\n",
    "        \"r2_test\": round(r2_te, 4),\n",
    "        \"rmse_test\": round(rmse_te, 2),\n",
    "        \"mae_test\": round(mae_te, 2),\n",
    "        \"overfit_gap_pct\": round(overfit_gap, 2),\n",
    "        \"R¬≤ OK?\": meets_r2,\n",
    "        \"RMSE OK?\": meets_rmse,\n",
    "        \"MAE OK?\": meets_mae,\n",
    "        \"Gap OK?\": meets_gap\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"üìä Comparison of candidate XGBoost configs (‚úÖ = meets criterion, ‚ùå = fails):\")\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6983db2a-2c92-4f82-8086-c162932d31db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Best model saved as xgb_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "best_xgb = Pipeline([\n",
    "    (\"pre\", preproc_tree),\n",
    "    (\"reg\", XGBRegressor(\n",
    "        n_estimators=250,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    ))\n",
    "])\n",
    "\n",
    "best_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Save to file\n",
    "joblib.dump(best_xgb, \"xgb_model.joblib\")\n",
    "print(\"‚úÖ Best model saved as xgb_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0260daed-4766-452e-a539-616f977b0821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved final pipeline as xgb_pipeline.joblib\n"
     ]
    }
   ],
   "source": [
    "# === finalize_model.py ===\n",
    "import joblib\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Use the preproc_tree we already defined in notebook\n",
    "best_xgb = Pipeline([\n",
    "    (\"pre\", preproc_tree),   # preprocessing pipeline\n",
    "    (\"reg\", XGBRegressor(\n",
    "        n_estimators=250,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the full dataset (better than just train split)\n",
    "best_xgb.fit(X, y)\n",
    "\n",
    "# Save the full pipeline\n",
    "joblib.dump(best_xgb, \"xgb_pipeline.joblib\")\n",
    "print(\"‚úÖ Saved final pipeline as xgb_pipeline.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f9208-56c7-477a-9b59-9d8c5ab90556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
